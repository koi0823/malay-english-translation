{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80cc443-377c-4cd4-877a-c19113a15103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sacrebleu import BLEU\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc58d48-89b7-48da-8438-8881d51e09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for memory optimization\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e3ec8a-ec6b-4f3b-b33f-9a9b5fde16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mesolitica/nanot5-small-malaysian-cased\"  \n",
    "DATA_PATH = \"/workspace/malaysian_english_stage2_part1\"  \n",
    "OUTPUT_DIR = \"/workspace/mt5_model\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 128\n",
    "MAX_TARGET_LENGTH = 128\n",
    "BATCH_SIZE = 64\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 3\n",
    "LOGGING_STEPS = 50\n",
    "EVAL_STEPS = 200\n",
    "SAVE_STEPS = 500\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba127ee7-8caa-4695-980b-ee66d4d7d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mt5_translation_run_rtx5090</strong> at: <a href='https://wandb.ai/koihaha/malay-english-translation/runs/0favemop' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation/runs/0favemop</a><br> View project at: <a href='https://wandb.ai/koihaha/malay-english-translation' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251025_092402-0favemop/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251025_092540-5sgfzxhl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/koihaha/malay-english-translation/runs/5sgfzxhl' target=\"_blank\">mt5_translation_run_rtx5090</a></strong> to <a href='https://wandb.ai/koihaha/malay-english-translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/koihaha/malay-english-translation' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/koihaha/malay-english-translation/runs/5sgfzxhl' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation/runs/5sgfzxhl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… W&B login successful and project initialized!\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"6afe11f68615fd2c34a35aa78d4b43e89001c527\")\n",
    "\n",
    "# âœ… Initialize your W&B project and run name\n",
    "wandb.init(\n",
    "    project=\"malay-english-translation\",\n",
    "    name=\"mt5_translation_run_rtx5090\",\n",
    "    config={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"max_length\": 128,\n",
    "        \"training\": \"seq2seq_trainer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ… W&B login successful and project initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c389ac9b-6a99-44a3-ae8c-4d133842661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"ğŸ“Š EVAL - Step {state.global_step}:\")\n",
    "            print(f\"   â€¢ Loss: {metrics.get('eval_loss', 'N/A')}\")\n",
    "            print(f\"   â€¢ BLEU: {metrics.get('eval_bleu', 'N/A')}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            print(f\"ğŸ“ˆ TRAIN - Step {state.global_step}: Loss = {logs['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd548be8-45e4-4824-b688-124ed314ad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Creating dataset directly from parquet...\n",
      "âœ… Loaded parquet: 173,799 rows, columns: ['src', 'tgt']\n",
      "âœ… Created dataset: 173,799 samples\n",
      "ğŸ“Š Original dataset: 173,799 samples\n",
      "ğŸ¯ Final dataset: 156,419 train, 17,380 test\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“¥ Creating dataset directly from parquet...\")\n",
    "parquet_path = os.path.join(DATA_PATH, \"malaysian_translation_stage2_part1.parquet\")\n",
    "\n",
    "try:\n",
    "    # Load the parquet file\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"âœ… Loaded parquet: {len(df):,} rows, columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Create dataset directly without saving to disk\n",
    "    full_dataset = Dataset.from_pandas(df)\n",
    "    print(f\"âœ… Created dataset: {len(full_dataset):,} samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading parquet: {e}\")\n",
    "    print(\"ğŸ“ Available files:\")\n",
    "    for file in os.listdir(DATA_PATH):\n",
    "        print(f\"   - {file}\")\n",
    "    raise\n",
    "\n",
    "print(f\"ğŸ“Š Original dataset: {len(full_dataset):,} samples\")\n",
    "\n",
    "# Split into train/validation\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"test\": split_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "print(f\"ğŸ¯ Final dataset: {len(final_dataset['train']):,} train, {len(final_dataset['test']):,} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc97b414-d737-4e02-9fee-7d33baafe745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Loading model and tokenizer...\n",
      "âœ… Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”§ Loading model and tokenizer...\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"âœ… Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115dab1f-f590-416e-9623-923e268a08e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f376ae57f943998ecf823bb7c049d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/156419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8840acf592047168f91c5954d914202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17380 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenization completed!\n"
     ]
    }
   ],
   "source": [
    "#tokenization before trainning\n",
    "def preprocess_function(examples):\n",
    "    inputs = [str(x) for x in examples[\"src\"]]\n",
    "    targets = [str(x) for x in examples[\"tgt\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"ğŸ”¹ Tokenizing dataset...\")\n",
    "tokenized_dataset = final_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=final_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"âœ… Tokenization completed!\")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e7f54-0702-42f3-afa5-8752d366dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLUE = measure how clsoe to a machine translation sentense to human\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    try:\n",
    "        # Clip predictions to valid token ID range\n",
    "        predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
    "        \n",
    "        # Replace -100 with pad token id in labels\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        #Token id turn back to human readable sentences\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Filter and clean\n",
    "        filtered_preds = []\n",
    "        filtered_refs = []\n",
    "        \n",
    "        for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "            pred_clean = pred.strip()\n",
    "            ref_clean = ref.strip()\n",
    "            \n",
    "            if pred_clean and ref_clean:\n",
    "                filtered_preds.append(pred_clean)\n",
    "                filtered_refs.append(ref_clean)\n",
    "        \n",
    "        if not filtered_preds:\n",
    "            return {\"bleu\": 0.0, \"gen_len\": 0.0}\n",
    "        \n",
    "        # Compute BLEU\n",
    "        bleu_scorer = BLEU(tokenize='13a')\n",
    "        bleu_result = bleu_scorer.corpus_score(filtered_preds, [filtered_refs])\n",
    "        \n",
    "        # Compute average generation length\n",
    "        prediction_lens = [len(pred.split()) for pred in filtered_preds]\n",
    "        avg_gen_len = np.mean(prediction_lens) if prediction_lens else 0.0\n",
    "        \n",
    "        return {\"bleu\": bleu_result.score, \"gen_len\": avg_gen_len}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ BLEU calculation failed: {e}\")\n",
    "        return {\"bleu\": 0.0, \"gen_len\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41eff2c-feb4-4c5f-822a-4a69549936d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING ARGUMENTS - OPTIMIZED FOR GPU UTILIZATION ===\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training - OPTIMIZED FOR GPU\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    #prevent overfitting\n",
    "    weight_decay=0.01,\n",
    "    #start with stable training\n",
    "    warmup_steps=500,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Mixed Precision for Better GPU Usage\n",
    "    fp16=True,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",  \n",
    "    eval_steps=EVAL_STEPS,  # 200\n",
    "    predict_with_generate=True,\n",
    "    \n",
    "    save_strategy=\"steps\",  \n",
    "    save_steps=400,  \n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_strategy=\"steps\",  \n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"nanot5-gpu-optimized\",\n",
    "    \n",
    "    # Optimization - GPU FOCUSED\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    remove_unused_columns=True,\n",
    "    label_names=[\"labels\"],\n",
    "    \n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "034ace19-2cae-4f4f-b7db-ede5f8425ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87915/1116464469.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[LoggingCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caffd698-054f-447f-a97e-779523a6b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ TRAINING CONFIGURATION:\n",
      "   â€¢ Dataset: 156,419 train samples\n",
      "   â€¢ Batch Size: 64 (effective: 64)\n",
      "   â€¢ Learning Rate: 0.0003\n",
      "   â€¢ Epochs: 3\n",
      "   â€¢ Total Steps: ~7,332\n"
     ]
    }
   ],
   "source": [
    "train_samples = len(tokenized_dataset[\"train\"])\n",
    "effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "steps_per_epoch = train_samples // effective_batch_size\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "\n",
    "print(f\"ğŸ¯ TRAINING CONFIGURATION:\")\n",
    "print(f\"   â€¢ Dataset: {train_samples:,} train samples\")\n",
    "print(f\"   â€¢ Batch Size: {BATCH_SIZE} (effective: {effective_batch_size})\")\n",
    "print(f\"   â€¢ Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   â€¢ Epochs: {EPOCHS}\")\n",
    "print(f\"   â€¢ Total Steps: ~{total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2443f104-f821-4761-a6ce-0d7b05eea95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ STARTING TRAINING...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7335' max='7335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7335/7335 1:34:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.214000</td>\n",
       "      <td>3.008575</td>\n",
       "      <td>0.195091</td>\n",
       "      <td>12.564557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.436400</td>\n",
       "      <td>2.366275</td>\n",
       "      <td>0.193646</td>\n",
       "      <td>13.697353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.119900</td>\n",
       "      <td>2.076819</td>\n",
       "      <td>0.226611</td>\n",
       "      <td>13.719448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.961200</td>\n",
       "      <td>1.935414</td>\n",
       "      <td>0.270594</td>\n",
       "      <td>14.126410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.892000</td>\n",
       "      <td>1.837950</td>\n",
       "      <td>0.268060</td>\n",
       "      <td>14.171692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.782800</td>\n",
       "      <td>1.769829</td>\n",
       "      <td>0.267691</td>\n",
       "      <td>13.963291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.743100</td>\n",
       "      <td>1.722468</td>\n",
       "      <td>0.282387</td>\n",
       "      <td>14.043843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>1.683757</td>\n",
       "      <td>0.262629</td>\n",
       "      <td>13.844074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.646300</td>\n",
       "      <td>1.649525</td>\n",
       "      <td>0.298375</td>\n",
       "      <td>14.088435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.638600</td>\n",
       "      <td>1.612836</td>\n",
       "      <td>0.289047</td>\n",
       "      <td>14.069390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>1.589442</td>\n",
       "      <td>0.287252</td>\n",
       "      <td>14.059666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.611200</td>\n",
       "      <td>1.569243</td>\n",
       "      <td>0.278768</td>\n",
       "      <td>13.882278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.513900</td>\n",
       "      <td>1.553184</td>\n",
       "      <td>0.318650</td>\n",
       "      <td>14.166916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.524600</td>\n",
       "      <td>1.534524</td>\n",
       "      <td>0.307037</td>\n",
       "      <td>14.029229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.529500</td>\n",
       "      <td>1.519979</td>\n",
       "      <td>0.283407</td>\n",
       "      <td>13.956732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.513200</td>\n",
       "      <td>1.504521</td>\n",
       "      <td>0.307490</td>\n",
       "      <td>13.998964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.491000</td>\n",
       "      <td>1.493607</td>\n",
       "      <td>0.289190</td>\n",
       "      <td>13.979517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.470100</td>\n",
       "      <td>1.481162</td>\n",
       "      <td>0.315114</td>\n",
       "      <td>14.109896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.469900</td>\n",
       "      <td>1.473118</td>\n",
       "      <td>0.301288</td>\n",
       "      <td>13.918815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.459000</td>\n",
       "      <td>1.461661</td>\n",
       "      <td>0.295997</td>\n",
       "      <td>13.933890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.451200</td>\n",
       "      <td>1.453693</td>\n",
       "      <td>0.308257</td>\n",
       "      <td>14.035731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.429100</td>\n",
       "      <td>1.445020</td>\n",
       "      <td>0.327369</td>\n",
       "      <td>14.061795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>1.439904</td>\n",
       "      <td>0.315755</td>\n",
       "      <td>14.049482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.417500</td>\n",
       "      <td>1.432387</td>\n",
       "      <td>0.313808</td>\n",
       "      <td>14.047353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.403600</td>\n",
       "      <td>1.425750</td>\n",
       "      <td>0.312188</td>\n",
       "      <td>13.959321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.399600</td>\n",
       "      <td>1.421236</td>\n",
       "      <td>0.291777</td>\n",
       "      <td>13.856387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.378300</td>\n",
       "      <td>1.415592</td>\n",
       "      <td>0.316704</td>\n",
       "      <td>13.970944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.414500</td>\n",
       "      <td>1.411361</td>\n",
       "      <td>0.304993</td>\n",
       "      <td>13.950288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.391500</td>\n",
       "      <td>1.408155</td>\n",
       "      <td>0.302347</td>\n",
       "      <td>13.971577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.362200</td>\n",
       "      <td>1.404670</td>\n",
       "      <td>0.314822</td>\n",
       "      <td>14.006041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.368500</td>\n",
       "      <td>1.401309</td>\n",
       "      <td>0.318183</td>\n",
       "      <td>13.986249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.361600</td>\n",
       "      <td>1.398477</td>\n",
       "      <td>0.298125</td>\n",
       "      <td>13.894994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.362500</td>\n",
       "      <td>1.396446</td>\n",
       "      <td>0.309604</td>\n",
       "      <td>13.983487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.362100</td>\n",
       "      <td>1.394915</td>\n",
       "      <td>0.314060</td>\n",
       "      <td>13.947699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.385500</td>\n",
       "      <td>1.393357</td>\n",
       "      <td>0.313714</td>\n",
       "      <td>13.960990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.354900</td>\n",
       "      <td>1.392596</td>\n",
       "      <td>0.313899</td>\n",
       "      <td>13.975834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ TRAIN - Step 50: Loss = 9.2281\n",
      "ğŸ“ˆ TRAIN - Step 100: Loss = 5.2174\n",
      "ğŸ“ˆ TRAIN - Step 150: Loss = 3.7644\n",
      "ğŸ“ˆ TRAIN - Step 200: Loss = 3.2140\n",
      "ğŸ“Š EVAL - Step 200:\n",
      "   â€¢ Loss: 3.008574962615967\n",
      "   â€¢ BLEU: 0.19509063968246593\n",
      "ğŸ“ˆ TRAIN - Step 250: Loss = 2.8953\n",
      "ğŸ“ˆ TRAIN - Step 300: Loss = 2.6861\n",
      "ğŸ“ˆ TRAIN - Step 350: Loss = 2.5537\n",
      "ğŸ“ˆ TRAIN - Step 400: Loss = 2.4364\n",
      "ğŸ“Š EVAL - Step 400:\n",
      "   â€¢ Loss: 2.3662750720977783\n",
      "   â€¢ BLEU: 0.19364599542687866\n",
      "ğŸ“ˆ TRAIN - Step 450: Loss = 2.3615\n",
      "ğŸ“ˆ TRAIN - Step 500: Loss = 2.2530\n",
      "ğŸ“ˆ TRAIN - Step 550: Loss = 2.1508\n",
      "ğŸ“ˆ TRAIN - Step 600: Loss = 2.1199\n",
      "ğŸ“Š EVAL - Step 600:\n",
      "   â€¢ Loss: 2.0768187046051025\n",
      "   â€¢ BLEU: 0.22661080403300918\n",
      "ğŸ“ˆ TRAIN - Step 650: Loss = 2.0528\n",
      "ğŸ“ˆ TRAIN - Step 700: Loss = 2.0468\n",
      "ğŸ“ˆ TRAIN - Step 750: Loss = 1.9769\n",
      "ğŸ“ˆ TRAIN - Step 800: Loss = 1.9612\n",
      "ğŸ“Š EVAL - Step 800:\n",
      "   â€¢ Loss: 1.9354138374328613\n",
      "   â€¢ BLEU: 0.270594482571157\n",
      "ğŸ“ˆ TRAIN - Step 850: Loss = 1.9357\n",
      "ğŸ“ˆ TRAIN - Step 900: Loss = 1.9100\n",
      "ğŸ“ˆ TRAIN - Step 950: Loss = 1.8958\n",
      "ğŸ“ˆ TRAIN - Step 1000: Loss = 1.8920\n",
      "ğŸ“Š EVAL - Step 1000:\n",
      "   â€¢ Loss: 1.8379497528076172\n",
      "   â€¢ BLEU: 0.2680603098762545\n",
      "ğŸ“ˆ TRAIN - Step 1050: Loss = 1.8453\n",
      "ğŸ“ˆ TRAIN - Step 1100: Loss = 1.8146\n",
      "ğŸ“ˆ TRAIN - Step 1150: Loss = 1.7713\n",
      "ğŸ“ˆ TRAIN - Step 1200: Loss = 1.7828\n",
      "ğŸ“Š EVAL - Step 1200:\n",
      "   â€¢ Loss: 1.7698293924331665\n",
      "   â€¢ BLEU: 0.2676912723074744\n",
      "ğŸ“ˆ TRAIN - Step 1250: Loss = 1.7708\n",
      "ğŸ“ˆ TRAIN - Step 1300: Loss = 1.7522\n",
      "ğŸ“ˆ TRAIN - Step 1350: Loss = 1.7507\n",
      "ğŸ“ˆ TRAIN - Step 1400: Loss = 1.7431\n",
      "ğŸ“Š EVAL - Step 1400:\n",
      "   â€¢ Loss: 1.7224681377410889\n",
      "   â€¢ BLEU: 0.2823871836972464\n",
      "ğŸ“ˆ TRAIN - Step 1450: Loss = 1.7215\n",
      "ğŸ“ˆ TRAIN - Step 1500: Loss = 1.6948\n",
      "ğŸ“ˆ TRAIN - Step 1550: Loss = 1.7087\n",
      "ğŸ“ˆ TRAIN - Step 1600: Loss = 1.7100\n",
      "ğŸ“Š EVAL - Step 1600:\n",
      "   â€¢ Loss: 1.6837574243545532\n",
      "   â€¢ BLEU: 0.26262855225284076\n",
      "ğŸ“ˆ TRAIN - Step 1650: Loss = 1.6823\n",
      "ğŸ“ˆ TRAIN - Step 1700: Loss = 1.6893\n",
      "ğŸ“ˆ TRAIN - Step 1750: Loss = 1.6521\n",
      "ğŸ“ˆ TRAIN - Step 1800: Loss = 1.6463\n",
      "ğŸ“Š EVAL - Step 1800:\n",
      "   â€¢ Loss: 1.6495252847671509\n",
      "   â€¢ BLEU: 0.2983747294361028\n",
      "ğŸ“ˆ TRAIN - Step 1850: Loss = 1.6715\n",
      "ğŸ“ˆ TRAIN - Step 1900: Loss = 1.6484\n",
      "ğŸ“ˆ TRAIN - Step 1950: Loss = 1.6096\n",
      "ğŸ“ˆ TRAIN - Step 2000: Loss = 1.6386\n",
      "ğŸ“Š EVAL - Step 2000:\n",
      "   â€¢ Loss: 1.6128360033035278\n",
      "   â€¢ BLEU: 0.2890470206263546\n",
      "ğŸ“ˆ TRAIN - Step 2050: Loss = 1.5764\n",
      "ğŸ“ˆ TRAIN - Step 2100: Loss = 1.5968\n",
      "ğŸ“ˆ TRAIN - Step 2150: Loss = 1.6090\n",
      "ğŸ“ˆ TRAIN - Step 2200: Loss = 1.6200\n",
      "ğŸ“Š EVAL - Step 2200:\n",
      "   â€¢ Loss: 1.5894417762756348\n",
      "   â€¢ BLEU: 0.2872515111811034\n",
      "ğŸ“ˆ TRAIN - Step 2250: Loss = 1.6025\n",
      "ğŸ“ˆ TRAIN - Step 2300: Loss = 1.5786\n",
      "ğŸ“ˆ TRAIN - Step 2350: Loss = 1.5693\n",
      "ğŸ“ˆ TRAIN - Step 2400: Loss = 1.6112\n",
      "ğŸ“Š EVAL - Step 2400:\n",
      "   â€¢ Loss: 1.56924307346344\n",
      "   â€¢ BLEU: 0.27876772593302057\n",
      "ğŸ“ˆ TRAIN - Step 2450: Loss = 1.5791\n",
      "ğŸ“ˆ TRAIN - Step 2500: Loss = 1.5535\n",
      "ğŸ“ˆ TRAIN - Step 2550: Loss = 1.5479\n",
      "ğŸ“ˆ TRAIN - Step 2600: Loss = 1.5139\n",
      "ğŸ“Š EVAL - Step 2600:\n",
      "   â€¢ Loss: 1.5531842708587646\n",
      "   â€¢ BLEU: 0.3186500298557451\n",
      "ğŸ“ˆ TRAIN - Step 2650: Loss = 1.5197\n",
      "ğŸ“ˆ TRAIN - Step 2700: Loss = 1.5448\n",
      "ğŸ“ˆ TRAIN - Step 2750: Loss = 1.5471\n",
      "ğŸ“ˆ TRAIN - Step 2800: Loss = 1.5246\n",
      "ğŸ“Š EVAL - Step 2800:\n",
      "   â€¢ Loss: 1.5345239639282227\n",
      "   â€¢ BLEU: 0.30703710131840706\n",
      "ğŸ“ˆ TRAIN - Step 2850: Loss = 1.5146\n",
      "ğŸ“ˆ TRAIN - Step 2900: Loss = 1.5282\n",
      "ğŸ“ˆ TRAIN - Step 2950: Loss = 1.5099\n",
      "ğŸ“ˆ TRAIN - Step 3000: Loss = 1.5295\n",
      "ğŸ“Š EVAL - Step 3000:\n",
      "   â€¢ Loss: 1.519979476928711\n",
      "   â€¢ BLEU: 0.28340714278840584\n",
      "ğŸ“ˆ TRAIN - Step 3050: Loss = 1.5463\n",
      "ğŸ“ˆ TRAIN - Step 3100: Loss = 1.5016\n",
      "ğŸ“ˆ TRAIN - Step 3150: Loss = 1.4971\n",
      "ğŸ“ˆ TRAIN - Step 3200: Loss = 1.5132\n",
      "ğŸ“Š EVAL - Step 3200:\n",
      "   â€¢ Loss: 1.504521131515503\n",
      "   â€¢ BLEU: 0.3074902628279294\n",
      "ğŸ“ˆ TRAIN - Step 3250: Loss = 1.4805\n",
      "ğŸ“ˆ TRAIN - Step 3300: Loss = 1.4884\n",
      "ğŸ“ˆ TRAIN - Step 3350: Loss = 1.5059\n",
      "ğŸ“ˆ TRAIN - Step 3400: Loss = 1.4910\n",
      "ğŸ“Š EVAL - Step 3400:\n",
      "   â€¢ Loss: 1.4936065673828125\n",
      "   â€¢ BLEU: 0.2891901535539226\n",
      "ğŸ“ˆ TRAIN - Step 3450: Loss = 1.4923\n",
      "ğŸ“ˆ TRAIN - Step 3500: Loss = 1.4534\n",
      "ğŸ“ˆ TRAIN - Step 3550: Loss = 1.4763\n",
      "ğŸ“ˆ TRAIN - Step 3600: Loss = 1.4701\n",
      "ğŸ“Š EVAL - Step 3600:\n",
      "   â€¢ Loss: 1.481162190437317\n",
      "   â€¢ BLEU: 0.31511353280486143\n",
      "ğŸ“ˆ TRAIN - Step 3650: Loss = 1.4565\n",
      "ğŸ“ˆ TRAIN - Step 3700: Loss = 1.4790\n",
      "ğŸ“ˆ TRAIN - Step 3750: Loss = 1.4421\n",
      "ğŸ“ˆ TRAIN - Step 3800: Loss = 1.4699\n",
      "ğŸ“Š EVAL - Step 3800:\n",
      "   â€¢ Loss: 1.473117709159851\n",
      "   â€¢ BLEU: 0.3012877304552581\n",
      "ğŸ“ˆ TRAIN - Step 3850: Loss = 1.4228\n",
      "ğŸ“ˆ TRAIN - Step 3900: Loss = 1.4667\n",
      "ğŸ“ˆ TRAIN - Step 3950: Loss = 1.4667\n",
      "ğŸ“ˆ TRAIN - Step 4000: Loss = 1.4590\n",
      "ğŸ“Š EVAL - Step 4000:\n",
      "   â€¢ Loss: 1.461661458015442\n",
      "   â€¢ BLEU: 0.2959968235948183\n",
      "ğŸ“ˆ TRAIN - Step 4050: Loss = 1.4622\n",
      "ğŸ“ˆ TRAIN - Step 4100: Loss = 1.4503\n",
      "ğŸ“ˆ TRAIN - Step 4150: Loss = 1.4601\n",
      "ğŸ“ˆ TRAIN - Step 4200: Loss = 1.4512\n",
      "ğŸ“Š EVAL - Step 4200:\n",
      "   â€¢ Loss: 1.4536930322647095\n",
      "   â€¢ BLEU: 0.30825747309810575\n",
      "ğŸ“ˆ TRAIN - Step 4250: Loss = 1.4525\n",
      "ğŸ“ˆ TRAIN - Step 4300: Loss = 1.4412\n",
      "ğŸ“ˆ TRAIN - Step 4350: Loss = 1.4522\n",
      "ğŸ“ˆ TRAIN - Step 4400: Loss = 1.4291\n",
      "ğŸ“Š EVAL - Step 4400:\n",
      "   â€¢ Loss: 1.4450196027755737\n",
      "   â€¢ BLEU: 0.3273690020174198\n",
      "ğŸ“ˆ TRAIN - Step 4450: Loss = 1.4168\n",
      "ğŸ“ˆ TRAIN - Step 4500: Loss = 1.4341\n",
      "ğŸ“ˆ TRAIN - Step 4550: Loss = 1.4548\n",
      "ğŸ“ˆ TRAIN - Step 4600: Loss = 1.4400\n",
      "ğŸ“Š EVAL - Step 4600:\n",
      "   â€¢ Loss: 1.439903736114502\n",
      "   â€¢ BLEU: 0.31575477597080776\n",
      "ğŸ“ˆ TRAIN - Step 4650: Loss = 1.4214\n",
      "ğŸ“ˆ TRAIN - Step 4700: Loss = 1.4578\n",
      "ğŸ“ˆ TRAIN - Step 4750: Loss = 1.4296\n",
      "ğŸ“ˆ TRAIN - Step 4800: Loss = 1.4175\n",
      "ğŸ“Š EVAL - Step 4800:\n",
      "   â€¢ Loss: 1.4323869943618774\n",
      "   â€¢ BLEU: 0.3138080900007328\n",
      "ğŸ“ˆ TRAIN - Step 4850: Loss = 1.4498\n",
      "ğŸ“ˆ TRAIN - Step 4900: Loss = 1.4081\n",
      "ğŸ“ˆ TRAIN - Step 4950: Loss = 1.4187\n",
      "ğŸ“ˆ TRAIN - Step 5000: Loss = 1.4036\n",
      "ğŸ“Š EVAL - Step 5000:\n",
      "   â€¢ Loss: 1.4257500171661377\n",
      "   â€¢ BLEU: 0.31218787581028756\n",
      "ğŸ“ˆ TRAIN - Step 5050: Loss = 1.3881\n",
      "ğŸ“ˆ TRAIN - Step 5100: Loss = 1.4287\n",
      "ğŸ“ˆ TRAIN - Step 5150: Loss = 1.3868\n",
      "ğŸ“ˆ TRAIN - Step 5200: Loss = 1.3996\n",
      "ğŸ“Š EVAL - Step 5200:\n",
      "   â€¢ Loss: 1.4212357997894287\n",
      "   â€¢ BLEU: 0.29177734224314283\n",
      "ğŸ“ˆ TRAIN - Step 5250: Loss = 1.3975\n",
      "ğŸ“ˆ TRAIN - Step 5300: Loss = 1.3898\n",
      "ğŸ“ˆ TRAIN - Step 5350: Loss = 1.3744\n",
      "ğŸ“ˆ TRAIN - Step 5400: Loss = 1.3783\n",
      "ğŸ“Š EVAL - Step 5400:\n",
      "   â€¢ Loss: 1.4155919551849365\n",
      "   â€¢ BLEU: 0.3167038147399998\n",
      "ğŸ“ˆ TRAIN - Step 5450: Loss = 1.3888\n",
      "ğŸ“ˆ TRAIN - Step 5500: Loss = 1.3960\n",
      "ğŸ“ˆ TRAIN - Step 5550: Loss = 1.3965\n",
      "ğŸ“ˆ TRAIN - Step 5600: Loss = 1.4145\n",
      "ğŸ“Š EVAL - Step 5600:\n",
      "   â€¢ Loss: 1.4113606214523315\n",
      "   â€¢ BLEU: 0.30499296249310054\n",
      "ğŸ“ˆ TRAIN - Step 5650: Loss = 1.4009\n",
      "ğŸ“ˆ TRAIN - Step 5700: Loss = 1.3833\n",
      "ğŸ“ˆ TRAIN - Step 5750: Loss = 1.3828\n",
      "ğŸ“ˆ TRAIN - Step 5800: Loss = 1.3915\n",
      "ğŸ“Š EVAL - Step 5800:\n",
      "   â€¢ Loss: 1.4081549644470215\n",
      "   â€¢ BLEU: 0.3023468085410247\n",
      "ğŸ“ˆ TRAIN - Step 5850: Loss = 1.3682\n",
      "ğŸ“ˆ TRAIN - Step 5900: Loss = 1.3942\n",
      "ğŸ“ˆ TRAIN - Step 5950: Loss = 1.4133\n",
      "ğŸ“ˆ TRAIN - Step 6000: Loss = 1.3622\n",
      "ğŸ“Š EVAL - Step 6000:\n",
      "   â€¢ Loss: 1.4046704769134521\n",
      "   â€¢ BLEU: 0.3148223474445167\n",
      "ğŸ“ˆ TRAIN - Step 6050: Loss = 1.3589\n",
      "ğŸ“ˆ TRAIN - Step 6100: Loss = 1.3972\n",
      "ğŸ“ˆ TRAIN - Step 6150: Loss = 1.3745\n",
      "ğŸ“ˆ TRAIN - Step 6200: Loss = 1.3685\n",
      "ğŸ“Š EVAL - Step 6200:\n",
      "   â€¢ Loss: 1.4013094902038574\n",
      "   â€¢ BLEU: 0.31818290891161044\n",
      "ğŸ“ˆ TRAIN - Step 6250: Loss = 1.3699\n",
      "ğŸ“ˆ TRAIN - Step 6300: Loss = 1.4157\n",
      "ğŸ“ˆ TRAIN - Step 6350: Loss = 1.3804\n",
      "ğŸ“ˆ TRAIN - Step 6400: Loss = 1.3616\n",
      "ğŸ“Š EVAL - Step 6400:\n",
      "   â€¢ Loss: 1.3984770774841309\n",
      "   â€¢ BLEU: 0.2981252014417743\n",
      "ğŸ“ˆ TRAIN - Step 6450: Loss = 1.3671\n",
      "ğŸ“ˆ TRAIN - Step 6500: Loss = 1.3865\n",
      "ğŸ“ˆ TRAIN - Step 6550: Loss = 1.3984\n",
      "ğŸ“ˆ TRAIN - Step 6600: Loss = 1.3625\n",
      "ğŸ“Š EVAL - Step 6600:\n",
      "   â€¢ Loss: 1.396445870399475\n",
      "   â€¢ BLEU: 0.30960437995990686\n",
      "ğŸ“ˆ TRAIN - Step 6650: Loss = 1.3900\n",
      "ğŸ“ˆ TRAIN - Step 6700: Loss = 1.3804\n",
      "ğŸ“ˆ TRAIN - Step 6750: Loss = 1.3700\n",
      "ğŸ“ˆ TRAIN - Step 6800: Loss = 1.3621\n",
      "ğŸ“Š EVAL - Step 6800:\n",
      "   â€¢ Loss: 1.3949148654937744\n",
      "   â€¢ BLEU: 0.3140600970656688\n",
      "ğŸ“ˆ TRAIN - Step 6850: Loss = 1.3804\n",
      "ğŸ“ˆ TRAIN - Step 6900: Loss = 1.4089\n",
      "ğŸ“ˆ TRAIN - Step 6950: Loss = 1.3552\n",
      "ğŸ“ˆ TRAIN - Step 7000: Loss = 1.3855\n",
      "ğŸ“Š EVAL - Step 7000:\n",
      "   â€¢ Loss: 1.3933570384979248\n",
      "   â€¢ BLEU: 0.31371358239513136\n",
      "ğŸ“ˆ TRAIN - Step 7050: Loss = 1.3958\n",
      "ğŸ“ˆ TRAIN - Step 7100: Loss = 1.3646\n",
      "ğŸ“ˆ TRAIN - Step 7150: Loss = 1.3877\n",
      "ğŸ“ˆ TRAIN - Step 7200: Loss = 1.3549\n",
      "ğŸ“Š EVAL - Step 7200:\n",
      "   â€¢ Loss: 1.3925962448120117\n",
      "   â€¢ BLEU: 0.3138987659480959\n",
      "ğŸ“ˆ TRAIN - Step 7250: Loss = 1.3754\n",
      "ğŸ“ˆ TRAIN - Step 7300: Loss = 1.3768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ STARTING TRAINING...\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "training_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7792314-f037-4aec-b8eb-df5274a450a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving model...\n",
      "ğŸ“ Model saved to: /workspace/mt5_model\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ’¾ Saving model...\")\n",
    "trainer.save_model()\n",
    "print(f\"ğŸ“ Model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1e82ad7-2684-4f09-b4cb-308ff9428236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Running final evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='544' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [544/544 02:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š EVAL - Step 7335:\n",
      "   â€¢ Loss: 1.4450196027755737\n",
      "   â€¢ BLEU: 0.3273690020174198\n",
      "ğŸ“Š Final BLEU score: 0.33\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>â–â–â–ƒâ–…â–…â–…â–†â–…â–†â–†â–†â–…â–ˆâ–‡â–†â–‡â–†â–‡â–‡â–†â–‡â–ˆâ–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–†â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>eval/gen_len</td><td>â–â–†â–†â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–‚â–„â–‚â–ƒâ–â–†â–ƒâ–ƒâ–‚â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‡â–†â–ˆâ–…â–…â–ƒâ–ƒâ–…â–‚â–‚â–‚â–…â–…â–†â–†â–‡â–…â–†â–…â–…â–†</td></tr><tr><td>eval/samples_per_second</td><td>â–‡â–…â–‡â–†â–ˆâ–ƒâ–†â–†â–‡â–…â–…â–…â–†â–†â–†â–†â–‚â–ƒâ–â–„â–„â–…â–†â–„â–‡â–‡â–‡â–„â–„â–ƒâ–ƒâ–‚â–„â–ƒâ–„â–„â–ƒ</td></tr><tr><td>eval/steps_per_second</td><td>â–‡â–…â–‡â–†â–ˆâ–ƒâ–†â–†â–‡â–…â–…â–…â–†â–†â–†â–†â–‚â–ƒâ–â–„â–„â–†â–†â–„â–‡â–‡â–‡â–„â–„â–ƒâ–ƒâ–‚â–„â–ƒâ–„â–„â–ƒ</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/learning_rate</td><td>â–ƒâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.32737</td></tr><tr><td>eval/gen_len</td><td>14.0618</td></tr><tr><td>eval/loss</td><td>1.44502</td></tr><tr><td>eval/runtime</td><td>143.8056</td></tr><tr><td>eval/samples_per_second</td><td>120.858</td></tr><tr><td>eval/steps_per_second</td><td>3.783</td></tr><tr><td>total_flos</td><td>2.6340507751809024e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>7335</td></tr><tr><td>train/grad_norm</td><td>5.86013</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mt5_translation_run_rtx5090</strong> at: <a href='https://wandb.ai/koihaha/malay-english-translation/runs/5sgfzxhl' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation/runs/5sgfzxhl</a><br> View project at: <a href='https://wandb.ai/koihaha/malay-english-translation' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251025_092540-5sgfzxhl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TRAINING COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Running final evaluation...\")\n",
    "final_metrics = trainer.evaluate()\n",
    "print(f\"ğŸ“Š Final BLEU score: {final_metrics.get('eval_bleu', 0):.2f}\")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"âœ… TRAINING COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ad639-1a51-4e9b-b29a-5d68c01e5dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
