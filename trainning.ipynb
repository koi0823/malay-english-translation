{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510a43cb-c7fe-4c96-b4f0-1aec4aef0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate sentencepiece wandb accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7848e5-c4d4-4850-a7fe-d516a888b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80cc443-377c-4cd4-877a-c19113a15103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23cdd98b-513e-4b11-94ba-5eec4b5acc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkclin01\u001b[0m (\u001b[33mkoihaha\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251024_112558-bte9wu9q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/koihaha/malay-english-translation/runs/bte9wu9q' target=\"_blank\">mt5_translation_run_rtx5090</a></strong> to <a href='https://wandb.ai/koihaha/malay-english-translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/koihaha/malay-english-translation' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/koihaha/malay-english-translation/runs/bte9wu9q' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation/runs/bte9wu9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ W&B login successful and project initialized!\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"6afe11f68615fd2c34a35aa78d4b43e89001c527\")\n",
    "\n",
    "# ‚úÖ Initialize your W&B project and run name\n",
    "wandb.init(\n",
    "    project=\"malay-english-translation\",\n",
    "    name=\"mt5_translation_run_rtx5090\",\n",
    "    config={\n",
    "        \"model\": \"google/mt5-base\",\n",
    "        \"batch_size\": 20,\n",
    "        \"epochs\": 5,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"max_length\": 256,\n",
    "        \"dataset_size\":\"5.8M\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ W&B login successful and project initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e3ec8a-ec6b-4f3b-b33f-9a9b5fde16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/mt5-base\"\n",
    "DATA_PATH = \"/workspace/malaysian_english\"\n",
    "OUTPUT_DIR = \"/workspace/mt5_model\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 256\n",
    "BATCH_SIZE = 20                     \n",
    "LEARNING_RATE = 2e-5                    \n",
    "EPOCHS = 5                              \n",
    "LOGGING_STEPS = 500\n",
    "SAVE_STEPS = 5000\n",
    "EVAL_STEPS = 5000\n",
    "GRADIENT_ACCUMULATION_STEPS = 3        \n",
    "WARMUP_STEPS = 5000                     \n",
    "LR_SCHEDULER_TYPE = \"cosine\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9996b357-9512-4165-a58a-37361956aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4566e79b-1278-4a4b-b000-695625e65a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading dataset from disk...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe483eaa2784b0796454795f0695f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split into train/test sets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src', 'tgt', 'prefix'],\n",
      "        num_rows: 5761747\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src', 'tgt', 'prefix'],\n",
      "        num_rows: 58200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#load dataset clened\n",
    "print(\"üîπ Loading dataset from disk...\")\n",
    "dataset = load_from_disk(DATA_PATH)\n",
    "\n",
    "if not isinstance(dataset, dict):\n",
    "    dataset = dataset.train_test_split(test_size=0.01, seed=42)\n",
    "    print(\"‚úÖ Split into train/test sets:\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f8e3c5-ae32-46b2-a012-f2094dfabf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Tokenizing dataset (this may take a while for 5.8M samples)...\n",
      "‚úÖ Tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [str(x) for x in examples[\"src\"]]\n",
    "    targets = [str(x) for x in examples[\"tgt\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"üîπ Tokenizing dataset (this may take a while for 5.8M samples)...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "print(\"‚úÖ Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4d9b9f7-4b23-4781-8888-653cb60e852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading model with memory optimizations...\n",
      "üíæ GPU Memory: 0.00 GB / 31.37 GB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"üíæ GPU Memory: {allocated:.2f} GB / {total:.2f} GB\")\n",
    "\n",
    "print(\"üîπ Loading model with memory optimizations...\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedeab30-b3df-47e8-a4fe-64b476cb4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Clear memory before training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Reduce memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2232986-fcbc-4105-ba1e-ad69518f68fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ GPU Memory: 2.17 GB / 31.37 GB\n"
     ]
    }
   ],
   "source": [
    "#load model and data collator\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# High-accuracy optimizations\n",
    "model.gradient_checkpointing_enable()  \n",
    "model.config.use_cache = False         \n",
    "print_gpu_memory()\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    pad_to_multiple_of=8  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46700cf-8591-4e01-837f-c09815743faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.10.23)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.1.2)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a48bc57e-7a70-40ce-a053-9bba147ff5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evalution metrics\n",
    "bleu = load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Handle both single references and multiple references\n",
    "    if isinstance(labels[0], list):\n",
    "        decoded_labels = [[tokenizer.decode(ref, skip_special_tokens=True) for ref in ref_list] for ref_list in labels]\n",
    "    else:\n",
    "        decoded_labels = [[tokenizer.decode(label, skip_special_tokens=True)] for label in labels]\n",
    "    \n",
    "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55f0bfc4-f4e5-4e18-aa82-f604e7f36d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c91562-0c35-4e7b-9e21-26897bd0566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=20,  \n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=3,   \n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                       \n",
    "    tf32=True,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    save_total_limit=2,  \n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"mt5_translation_run_rtx5090\",\n",
    "    dataloader_num_workers=4,        \n",
    "    dataloader_prefetch_factor=1,    \n",
    "    dataloader_pin_memory=False,     \n",
    "    logging_steps=2000,              \n",
    "    remove_unused_columns=True,\n",
    "    resume_from_checkpoint=False,  \n",
    "    gradient_checkpointing=False,    \n",
    "    optim=\"adamw_torch_fused\",       \n",
    "    logging_first_step=False,\n",
    "    dataloader_drop_last=True,\n",
    "    skip_memory_metrics=True,        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d37642e3-83ab-42c1-8b2a-778446f4744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "#trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7d5eb-8d4d-42d3-bce7-dbcd61ec3a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ HIGH ACCURACY TRAINING CONFIGURATION:\n",
      "   ‚Ä¢ Dataset: 5,761,747 samples\n",
      "   ‚Ä¢ Batch Size: 20 (per device)\n",
      "   ‚Ä¢ Effective Batch: 60 (accumulated)\n",
      "   ‚Ä¢ Learning Rate: 2e-05 (optimal for stability)\n",
      "   ‚Ä¢ Epochs: 5\n",
      "   ‚Ä¢ Total Steps: ~480,145\n",
      "   ‚Ä¢ Warmup: 5000 steps\n",
      "   ‚Ä¢ Evaluation: Every 5000 steps\n",
      "üíæ GPU Memory: 2.17 GB / 31.37 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='480145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5001/480145 56:27 < 89:26:07, 1.48 it/s, Epoch 0.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='642' max='7275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 642/7275 03:36 < 37:20, 2.96 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#start train\n",
    "train_samples = len(tokenized_dataset[\"train\"])\n",
    "effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "steps_per_epoch = train_samples // effective_batch_size\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "\n",
    "print(f\"üéØ HIGH ACCURACY TRAINING CONFIGURATION:\")\n",
    "print(f\"   ‚Ä¢ Dataset: {train_samples:,} samples\")\n",
    "print(f\"   ‚Ä¢ Batch Size: {BATCH_SIZE} (per device)\")\n",
    "print(f\"   ‚Ä¢ Effective Batch: {effective_batch_size} (accumulated)\")\n",
    "print(f\"   ‚Ä¢ Learning Rate: {LEARNING_RATE} (optimal for stability)\")\n",
    "print(f\"   ‚Ä¢ Epochs: {EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ Total Steps: ~{total_steps:,}\")\n",
    "print(f\"   ‚Ä¢ Warmup: {WARMUP_STEPS} steps\")\n",
    "print(f\"   ‚Ä¢ Evaluation: Every {EVAL_STEPS} steps\")\n",
    "\n",
    "print_gpu_memory()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28668d87-7df7-4646-b0a4-f00caa133b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final model\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/high_accuracy_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/high_accuracy_model\")\n",
    "\n",
    "print(f\"üìÅ Model saved to: {OUTPUT_DIR}/high_accuracy_model\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from sacrebleu import BLEU\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from evaluate import load\n",
    "\n",
    "# Set environment variables for memory optimization\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "MODEL_NAME = \"mesolitica/nanot5-small-malaysian-cased\"  \n",
    "DATA_PATH = \"/workspace/malaysian_english_stage2_part1\"  \n",
    "OUTPUT_DIR = \"/workspace/mt5_model\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 64\n",
    "MAX_TARGET_LENGTH = 64\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 1\n",
    "LOGGING_STEPS = 50\n",
    "EVAL_STEPS = 200\n",
    "SAVE_STEPS = 500\n",
    "          \n",
    "wandb.login(key=\"6afe11f68615fd2c34a35aa78d4b43e89001c527\")\n",
    "\n",
    "# ‚úÖ Initialize your W&B project and run name\n",
    "wandb.init(\n",
    "    project=\"malay-english-translation\",\n",
    "    name=\"mt5_translation_run_rtx5090\",\n",
    "    config={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"max_length\": 128,\n",
    "        \"training\": \"seq2seq_trainer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ W&B login successful and project initialized!\")\n",
    "\n",
    "from transformers import TrainerCallback  # ‚Üê ADD THIS IMPORT\n",
    "\n",
    "class DebugCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"üîç EVAL DEBUG - Step {state.global_step}:\")\n",
    "            print(f\"   ‚Ä¢ Loss: {metrics.get('eval_loss', 'N/A')}\")\n",
    "            print(f\"   ‚Ä¢ BLEU: {metrics.get('eval_bleu', 'N/A')}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            print(f\"üìà TRAIN - Step {state.global_step}: Loss = {logs['loss']:.4f}\")\n",
    "\n",
    "# === DATA LOADING ===\n",
    "def load_fresh_dataset():\n",
    "    \"\"\"Load dataset directly from Hugging Face\"\"\"\n",
    "    print(\"üì• Loading fresh dataset from Hugging Face...\")\n",
    "    \n",
    "    try:\n",
    "        # Load directly from the source\n",
    "        dataset = load_dataset(\n",
    "            \"mesolitica/Malaysian-Translation\", \n",
    "            \"stage2-part1\",\n",
    "            split='train'\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Original dataset: {len(dataset):,} samples\")\n",
    "        \n",
    "        # Add prefixes (since we need this preprocessing)\n",
    "        def add_prefix(example):\n",
    "            src = str(example['src'])\n",
    "            # Simple language detection for prefix\n",
    "            if any('\\u4e00' <= char <= '\\u9fff' for char in src):\n",
    "                prefix = 'terjemah Cina ke Bahasa Melayu: '\n",
    "            elif any('\\u0b80' <= char <= '\\u0bff' for char in src):\n",
    "                prefix = 'terjemah Tamil ke Bahasa Melayu: '\n",
    "            elif any(char in '⁄Ω⁄¨⁄†›¢€è⁄î⁄é⁄É' for char in src):\n",
    "                prefix = 'terjemah Jawi ke Bahasa Melayu: '\n",
    "            else:\n",
    "                prefix = 'terjemah Inggeris ke Bahasa Melayu: '\n",
    "            \n",
    "            example['src'] = prefix + src\n",
    "            return example\n",
    "        \n",
    "        # Apply prefixes to first 60k samples for faster processing\n",
    "        print(\"üè∑Ô∏è Adding language prefixes...\")\n",
    "        dataset = dataset.select(range(60000)).map(add_prefix)\n",
    "        \n",
    "        print(f\"‚úÖ Processed dataset: {len(dataset):,} samples\")\n",
    "        return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading from HF: {e}\")\n",
    "        return create_fallback_dataset()\n",
    "\n",
    "def create_fallback_dataset():\n",
    "    \"\"\"Create a fallback dataset if HF loading fails\"\"\"\n",
    "    print(\"üîÑ Creating fallback dataset...\")\n",
    "    \n",
    "    # Sample translation pairs for immediate training\n",
    "    test_data = {\n",
    "        \"src\": [\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Hello, how are you?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Good morning everyone\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: What is your name?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Thank you very much\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Where is the restaurant?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: I would like to order food\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: How much does this cost?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Can you help me please?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: What time is it now?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: I need to go to the hospital\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: The weather is very nice today\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: How do I get to the airport?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: What is this for?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: I don't understand\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Please speak slowly\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"Hello, apa khabar?\",\n",
    "            \"Selamat pagi semua\",\n",
    "            \"Siapa nama awak?\",\n",
    "            \"Terima kasih banyak-banyak\",\n",
    "            \"Mana restoran?\",\n",
    "            \"Saya nak order makanan\",\n",
    "            \"Berapa harga ini?\",\n",
    "            \"Boleh tolong saya?\",\n",
    "            \"Pukul berapa sekarang?\",\n",
    "            \"Saya perlu pergi ke hospital\",\n",
    "            \"Cuaca hari ini sangat cantik\",\n",
    "            \"Macam mana nak pergi ke lapangan terbang?\",\n",
    "            \"Ini untuk apa?\",\n",
    "            \"Saya tidak faham\",\n",
    "            \"Tolong cakap perlahan-lahan\",\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Create larger dataset by repeating\n",
    "    expanded_src = []\n",
    "    expanded_tgt = []\n",
    "    for i in range(100):  # 1500 samples total\n",
    "        expanded_src.extend(test_data[\"src\"])\n",
    "        expanded_tgt.extend(test_data[\"tgt\"])\n",
    "    \n",
    "    dataset = Dataset.from_dict({\n",
    "        \"src\": expanded_src,\n",
    "        \"tgt\": expanded_tgt\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Fallback dataset: {len(dataset):,} samples\")\n",
    "    return dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_fresh_dataset()\n",
    "\n",
    "# Split into train/test\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Use appropriate sizes\n",
    "train_size = min(50000, len(split_dataset[\"train\"]))\n",
    "test_size = min(5000, len(split_dataset[\"test\"]))\n",
    "\n",
    "train_subset = split_dataset[\"train\"].select(range(train_size))\n",
    "test_subset = split_dataset[\"test\"].select(range(test_size))\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"test\": test_subset\n",
    "})\n",
    "\n",
    "print(f\"üéØ Final dataset: {len(train_subset):,} train, {len(test_subset):,} test\")\n",
    "\n",
    "print(\"üîß Loading model and tokenizer...\")\n",
    "\n",
    "# Clear GPU memory first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load model with memory optimizations\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float16,  # Load in half precision\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded successfully!\")\n",
    "\n",
    "def check_training_data():\n",
    "    \"\"\"Check if training data looks correct\"\"\"\n",
    "    print(\"üîç Checking training data samples...\")\n",
    "    \n",
    "    sample = final_dataset[\"train\"][0]\n",
    "    input_text = sample[\"src\"]\n",
    "    target_text = sample[\"tgt\"]\n",
    "    \n",
    "    print(f\"üìù Input: {input_text[:100]}...\")\n",
    "    print(f\"üéØ Target: {target_text[:100]}...\")\n",
    "    print(f\"üìä Input length: {len(input_text)}, Target length: {len(target_text)}\")\n",
    "\n",
    "check_training_data()\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize the examples\"\"\"\n",
    "    inputs = [str(x) for x in examples[\"src\"]]\n",
    "    targets = [str(x) for x in examples[\"tgt\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"üîπ Tokenizing dataset...\")\n",
    "tokenized_dataset = final_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=final_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization completed!\")\n",
    "\n",
    "# === DATA COLLATOR ===\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute BLEU score for evaluation - WITH OVERFLOW PROTECTION\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    try:\n",
    "        # **FIX: Clip predictions to valid token ID range**\n",
    "        predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
    "        \n",
    "        # Replace -100 with pad token id in labels\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        # Decode predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Filter and clean\n",
    "        filtered_preds = []\n",
    "        filtered_refs = []\n",
    "        \n",
    "        for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "            pred_clean = pred.strip()\n",
    "            ref_clean = ref.strip()\n",
    "            \n",
    "            if pred_clean and ref_clean:\n",
    "                filtered_preds.append(pred_clean)\n",
    "                filtered_refs.append(ref_clean)\n",
    "        \n",
    "        if not filtered_preds:\n",
    "            return {\"bleu\": 0.0, \"gen_len\": 0.0}\n",
    "        \n",
    "        # Compute BLEU\n",
    "        bleu_scorer = BLEU(tokenize='13a')\n",
    "        bleu_result = bleu_scorer.corpus_score(filtered_preds, [filtered_refs])\n",
    "        \n",
    "        # Compute average generation length\n",
    "        prediction_lens = [len(pred.split()) for pred in filtered_preds]\n",
    "        avg_gen_len = np.mean(prediction_lens) if prediction_lens else 0.0\n",
    "        \n",
    "        print(f\"‚úÖ BLEU: {bleu_result.score:.4f}, Samples: {len(filtered_preds)}, Avg Len: {avg_gen_len:.1f}\")\n",
    "        \n",
    "        return {\"bleu\": bleu_result.score, \"gen_len\": avg_gen_len}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BLEU calculation failed: {e}\")\n",
    "        return {\"bleu\": 0.0, \"gen_len\": 0.0}\n",
    "\n",
    "def check_model_outputs():\n",
    "    \"\"\"Check if model is producing reasonable outputs\"\"\"\n",
    "    print(\"üîç Checking model outputs...\")\n",
    "    \n",
    "    # Move model to GPU first\n",
    "    model.to(\"cuda\")\n",
    "    print(\"‚úÖ Model moved to GPU\")\n",
    "    \n",
    "    # Test with a simple example\n",
    "    test_input = \"terjemah Inggeris ke Bahasa Melayu: Hello world\"\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\", max_length=128, truncation=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=50,\n",
    "            num_beams=1,  # Use greedy for quick test\n",
    "            # Remove early_stopping for greedy search\n",
    "        )\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"üìù Test input: {test_input}\")\n",
    "    print(f\"üéØ Model output: {prediction}\")\n",
    "    \n",
    "    # Check if output is reasonable\n",
    "    if len(prediction.strip()) > 0 and prediction != test_input:\n",
    "        print(\"‚úÖ Model is producing different outputs\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Model output issue detected\")\n",
    "        return False\n",
    "\n",
    "# Run the check\n",
    "check_model_outputs()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # TRAINING - ULTRA STABLE\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,           # Mild regularization\n",
    "    warmup_steps=1000,           # More warmup\n",
    "    max_grad_norm=0.5,           # Conservative gradient clipping\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",  \n",
    "    eval_steps=EVAL_STEPS,       # 200\n",
    "    predict_with_generate=True,\n",
    "    \n",
    "    # Saving - FIXED: Make save_steps multiple of eval_steps\n",
    "    save_strategy=\"steps\",  \n",
    "    save_steps=400,              # ‚Üê CHANGED from 500 to 400 (multiple of 200)\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_strategy=\"steps\",  \n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"nanot5-small-stable-run\",\n",
    "    \n",
    "    # Optimization - STABILITY FIRST\n",
    "    fp16=False,                 # NO mixed precision\n",
    "    bf16=False,                 # NO mixed precision  \n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[DebugCallback()]  # Add debug callback\n",
    ")\n",
    "\n",
    "train_samples = len(tokenized_dataset[\"train\"])\n",
    "effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "steps_per_epoch = train_samples // effective_batch_size\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "\n",
    "print(f\"üéØ FINAL TRAINING CONFIGURATION:\")\n",
    "print(f\"   ‚Ä¢ Model: {MODEL_NAME}\")\n",
    "print(f\"   ‚Ä¢ Dataset: {train_samples:,} train, {len(tokenized_dataset['test']):,} test\")\n",
    "print(f\"   ‚Ä¢ Batch Size: {BATCH_SIZE} (effective: {effective_batch_size})\")\n",
    "print(f\"   ‚Ä¢ Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ Epochs: {EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ Total Steps: ~{total_steps:,}\")\n",
    "print(f\"   ‚Ä¢ Evaluation: Every {EVAL_STEPS} steps\")\n",
    "print(f\"   ‚Ä¢ Target BLEU: >10.0\")\n",
    "\n",
    "# Clear everything first\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"üîÑ Loading nanot5-small with stability checks...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load in FP32 for maximum stability\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float32,  # FP32 ONLY for stability\n",
    ")\n",
    "\n",
    "print(\"‚úÖ nanot5-small loaded successfully!\")\n",
    "print(f\"üìä Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "def quick_stability_check():\n",
    "    \"\"\"Quick stability test for T5 models\"\"\"\n",
    "    print(\"üîç Quick stability check for T5...\")\n",
    "    \n",
    "    try:\n",
    "        # Move model to GPU\n",
    "        model.to(\"cuda\")\n",
    "        \n",
    "        # Test input and target (T5 needs both for forward pass)\n",
    "        test_input = \"terjemah Inggeris ke Bahasa Melayu: Hello world\"\n",
    "        test_target = \"Halo dunia\"\n",
    "        \n",
    "        # Tokenize both input and target\n",
    "        inputs = tokenizer(test_input, return_tensors=\"pt\", max_length=64, truncation=True, padding=\"max_length\")\n",
    "        labels = tokenizer(test_target, return_tensors=\"pt\", max_length=64, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        # Remove token_type_ids for T5\n",
    "        for d in [inputs, labels]:\n",
    "            if 'token_type_ids' in d:\n",
    "                del d['token_type_ids']\n",
    "        \n",
    "        # Move to GPU\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        labels = labels['input_ids'].to(\"cuda\")\n",
    "        \n",
    "        # For T5 forward pass with loss, we need to provide labels\n",
    "        inputs['labels'] = labels\n",
    "        \n",
    "        # Test forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss.item() if outputs.loss is not None else \"N/A\"\n",
    "        \n",
    "        print(f\"üìä Forward loss: {loss}\")\n",
    "        \n",
    "        # Test generation separately (this doesn't need labels)\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                max_length=32,\n",
    "                num_beams=1\n",
    "            )\n",
    "            prediction = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"üéØ Test generation: {prediction}\")\n",
    "        \n",
    "        # Check if reasonable\n",
    "        if isinstance(loss, float) and loss < 1000 and loss > 0:\n",
    "            print(\"‚úÖ Stability check passed!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Loss issue detected\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during stability check: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the check\n",
    "if quick_stability_check():\n",
    "    print(\"üéâ Ready for stable training!\")\n",
    "else:\n",
    "    print(\"üö® Stability issues detected!\")\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset_path = \"/workspace/malaysian_english_stage2_part1\"\n",
    "\n",
    "print(\"üîÑ Attempting to recreate from parquet file...\")\n",
    "try:\n",
    "    # Load the parquet file that's in your directory\n",
    "    parquet_path = os.path.join(dataset_path, \"malaysian_translation_stage2_part1.parquet\")\n",
    "    \n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"üìñ Loading parquet file: {parquet_path}\")\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        print(f\"‚úÖ Loaded parquet: {len(df):,} rows, columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Convert to Hugging Face dataset\n",
    "        full_dataset = Dataset.from_pandas(df)\n",
    "        print(f\"‚úÖ Created dataset: {len(full_dataset):,} samples\")\n",
    "        \n",
    "        # Save it properly\n",
    "        print(\"üíæ Saving as proper dataset...\")\n",
    "        full_dataset.save_to_disk(dataset_path + \"_fixed\")\n",
    "        print(\"‚úÖ Dataset saved successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Parquet file not found at: {parquet_path}\")\n",
    "        print(\"üìÅ Available files:\")\n",
    "        for file in os.listdir(dataset_path):\n",
    "            print(f\"   - {file}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error recreating from parquet: {e}\")\n",
    "\n",
    "print(\"üöÄ STARTING TRAINING...\")\n",
    "torch.cuda.empty_cache()\n",
    "training_result = trainer.train()\n",
    "\n",
    "# Start training (this will run for several hours)\n",
    "training_result = trainer.train()\n",
    "\n",
    "# === SAVE MODEL ===\n",
    "print(\"üíæ Saving model...\")\n",
    "trainer.save_model()\n",
    "\n",
    "print(f\"üìÅ Model saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# === FINAL EVALUATION ===\n",
    "print(\"üîç Running final evaluation...\")\n",
    "final_metrics = trainer.evaluate()\n",
    "print(f\"üìä Final BLEU score: {final_metrics.get('eval_bleu', 0):.2f}\")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "\n",
    "okie now is to fix the full dataset and remove those debug code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
