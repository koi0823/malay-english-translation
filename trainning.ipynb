{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510a43cb-c7fe-4c96-b4f0-1aec4aef0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate sentencepiece wandb accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7848e5-c4d4-4850-a7fe-d516a888b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80cc443-377c-4cd4-877a-c19113a15103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23cdd98b-513e-4b11-94ba-5eec4b5acc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkclin01\u001b[0m (\u001b[33mkoihaha\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251024_112558-bte9wu9q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/koihaha/malay-english-translation/runs/bte9wu9q' target=\"_blank\">mt5_translation_run_rtx5090</a></strong> to <a href='https://wandb.ai/koihaha/malay-english-translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/koihaha/malay-english-translation' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/koihaha/malay-english-translation/runs/bte9wu9q' target=\"_blank\">https://wandb.ai/koihaha/malay-english-translation/runs/bte9wu9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ W&B login successful and project initialized!\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"6afe11f68615fd2c34a35aa78d4b43e89001c527\")\n",
    "\n",
    "# ✅ Initialize your W&B project and run name\n",
    "wandb.init(\n",
    "    project=\"malay-english-translation\",\n",
    "    name=\"mt5_translation_run_rtx5090\",\n",
    "    config={\n",
    "        \"model\": \"google/mt5-base\",\n",
    "        \"batch_size\": 20,\n",
    "        \"epochs\": 5,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"max_length\": 256,\n",
    "        \"dataset_size\":\"5.8M\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ W&B login successful and project initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e3ec8a-ec6b-4f3b-b33f-9a9b5fde16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/mt5-base\"\n",
    "DATA_PATH = \"/workspace/malaysian_english\"\n",
    "OUTPUT_DIR = \"/workspace/mt5_model\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 256\n",
    "BATCH_SIZE = 20                     \n",
    "LEARNING_RATE = 2e-5                    \n",
    "EPOCHS = 5                              \n",
    "LOGGING_STEPS = 500\n",
    "SAVE_STEPS = 5000\n",
    "EVAL_STEPS = 5000\n",
    "GRADIENT_ACCUMULATION_STEPS = 3        \n",
    "WARMUP_STEPS = 5000                     \n",
    "LR_SCHEDULER_TYPE = \"cosine\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9996b357-9512-4165-a58a-37361956aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4566e79b-1278-4a4b-b000-695625e65a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Loading dataset from disk...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe483eaa2784b0796454795f0695f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split into train/test sets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src', 'tgt', 'prefix'],\n",
      "        num_rows: 5761747\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src', 'tgt', 'prefix'],\n",
      "        num_rows: 58200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#load dataset clened\n",
    "print(\"🔹 Loading dataset from disk...\")\n",
    "dataset = load_from_disk(DATA_PATH)\n",
    "\n",
    "if not isinstance(dataset, dict):\n",
    "    dataset = dataset.train_test_split(test_size=0.01, seed=42)\n",
    "    print(\"✅ Split into train/test sets:\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f8e3c5-ae32-46b2-a012-f2094dfabf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Tokenizing dataset (this may take a while for 5.8M samples)...\n",
      "✅ Tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [str(x) for x in examples[\"src\"]]\n",
    "    targets = [str(x) for x in examples[\"tgt\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"🔹 Tokenizing dataset (this may take a while for 5.8M samples)...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "print(\"✅ Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4d9b9f7-4b23-4781-8888-653cb60e852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Loading model with memory optimizations...\n",
      "💾 GPU Memory: 0.00 GB / 31.37 GB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"💾 GPU Memory: {allocated:.2f} GB / {total:.2f} GB\")\n",
    "\n",
    "print(\"🔹 Loading model with memory optimizations...\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedeab30-b3df-47e8-a4fe-64b476cb4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Clear memory before training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Reduce memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2232986-fcbc-4105-ba1e-ad69518f68fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 GPU Memory: 2.17 GB / 31.37 GB\n"
     ]
    }
   ],
   "source": [
    "#load model and data collator\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# High-accuracy optimizations\n",
    "model.gradient_checkpointing_enable()  \n",
    "model.config.use_cache = False         \n",
    "print_gpu_memory()\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    pad_to_multiple_of=8  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46700cf-8591-4e01-837f-c09815743faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.10.23)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.1.2)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a48bc57e-7a70-40ce-a053-9bba147ff5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evalution metrics\n",
    "bleu = load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Handle both single references and multiple references\n",
    "    if isinstance(labels[0], list):\n",
    "        decoded_labels = [[tokenizer.decode(ref, skip_special_tokens=True) for ref in ref_list] for ref_list in labels]\n",
    "    else:\n",
    "        decoded_labels = [[tokenizer.decode(label, skip_special_tokens=True)] for label in labels]\n",
    "    \n",
    "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55f0bfc4-f4e5-4e18-aa82-f604e7f36d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c91562-0c35-4e7b-9e21-26897bd0566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=20,  \n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=3,   \n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                       \n",
    "    tf32=True,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    save_total_limit=2,  \n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"mt5_translation_run_rtx5090\",\n",
    "    dataloader_num_workers=4,        \n",
    "    dataloader_prefetch_factor=1,    \n",
    "    dataloader_pin_memory=False,     \n",
    "    logging_steps=2000,              \n",
    "    remove_unused_columns=True,\n",
    "    resume_from_checkpoint=False,  \n",
    "    gradient_checkpointing=False,    \n",
    "    optim=\"adamw_torch_fused\",       \n",
    "    logging_first_step=False,\n",
    "    dataloader_drop_last=True,\n",
    "    skip_memory_metrics=True,        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d37642e3-83ab-42c1-8b2a-778446f4744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "#trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7d5eb-8d4d-42d3-bce7-dbcd61ec3a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 HIGH ACCURACY TRAINING CONFIGURATION:\n",
      "   • Dataset: 5,761,747 samples\n",
      "   • Batch Size: 20 (per device)\n",
      "   • Effective Batch: 60 (accumulated)\n",
      "   • Learning Rate: 2e-05 (optimal for stability)\n",
      "   • Epochs: 5\n",
      "   • Total Steps: ~480,145\n",
      "   • Warmup: 5000 steps\n",
      "   • Evaluation: Every 5000 steps\n",
      "💾 GPU Memory: 2.17 GB / 31.37 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='480145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5001/480145 56:27 < 89:26:07, 1.48 it/s, Epoch 0.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='642' max='7275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 642/7275 03:36 < 37:20, 2.96 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#start train\n",
    "train_samples = len(tokenized_dataset[\"train\"])\n",
    "effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "steps_per_epoch = train_samples // effective_batch_size\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "\n",
    "print(f\"🎯 HIGH ACCURACY TRAINING CONFIGURATION:\")\n",
    "print(f\"   • Dataset: {train_samples:,} samples\")\n",
    "print(f\"   • Batch Size: {BATCH_SIZE} (per device)\")\n",
    "print(f\"   • Effective Batch: {effective_batch_size} (accumulated)\")\n",
    "print(f\"   • Learning Rate: {LEARNING_RATE} (optimal for stability)\")\n",
    "print(f\"   • Epochs: {EPOCHS}\")\n",
    "print(f\"   • Total Steps: ~{total_steps:,}\")\n",
    "print(f\"   • Warmup: {WARMUP_STEPS} steps\")\n",
    "print(f\"   • Evaluation: Every {EVAL_STEPS} steps\")\n",
    "\n",
    "print_gpu_memory()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28668d87-7df7-4646-b0a4-f00caa133b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final model\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/high_accuracy_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/high_accuracy_model\")\n",
    "\n",
    "print(f\"📁 Model saved to: {OUTPUT_DIR}/high_accuracy_model\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from sacrebleu import BLEU\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from evaluate import load\n",
    "\n",
    "# Set environment variables for memory optimization\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "MODEL_NAME = \"mesolitica/nanot5-small-malaysian-cased\"  \n",
    "DATA_PATH = \"/workspace/malaysian_english_stage2_part1\"  \n",
    "OUTPUT_DIR = \"/workspace/mt5_model\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 64\n",
    "MAX_TARGET_LENGTH = 64\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 1\n",
    "LOGGING_STEPS = 50\n",
    "EVAL_STEPS = 200\n",
    "SAVE_STEPS = 500\n",
    "          \n",
    "wandb.login(key=\"6afe11f68615fd2c34a35aa78d4b43e89001c527\")\n",
    "\n",
    "# ✅ Initialize your W&B project and run name\n",
    "wandb.init(\n",
    "    project=\"malay-english-translation\",\n",
    "    name=\"mt5_translation_run_rtx5090\",\n",
    "    config={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"max_length\": 128,\n",
    "        \"training\": \"seq2seq_trainer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ W&B login successful and project initialized!\")\n",
    "\n",
    "from transformers import TrainerCallback  # ← ADD THIS IMPORT\n",
    "\n",
    "class DebugCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"🔍 EVAL DEBUG - Step {state.global_step}:\")\n",
    "            print(f\"   • Loss: {metrics.get('eval_loss', 'N/A')}\")\n",
    "            print(f\"   • BLEU: {metrics.get('eval_bleu', 'N/A')}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            print(f\"📈 TRAIN - Step {state.global_step}: Loss = {logs['loss']:.4f}\")\n",
    "\n",
    "# === DATA LOADING ===\n",
    "def load_fresh_dataset():\n",
    "    \"\"\"Load dataset directly from Hugging Face\"\"\"\n",
    "    print(\"📥 Loading fresh dataset from Hugging Face...\")\n",
    "    \n",
    "    try:\n",
    "        # Load directly from the source\n",
    "        dataset = load_dataset(\n",
    "            \"mesolitica/Malaysian-Translation\", \n",
    "            \"stage2-part1\",\n",
    "            split='train'\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Original dataset: {len(dataset):,} samples\")\n",
    "        \n",
    "        # Add prefixes (since we need this preprocessing)\n",
    "        def add_prefix(example):\n",
    "            src = str(example['src'])\n",
    "            # Simple language detection for prefix\n",
    "            if any('\\u4e00' <= char <= '\\u9fff' for char in src):\n",
    "                prefix = 'terjemah Cina ke Bahasa Melayu: '\n",
    "            elif any('\\u0b80' <= char <= '\\u0bff' for char in src):\n",
    "                prefix = 'terjemah Tamil ke Bahasa Melayu: '\n",
    "            elif any(char in 'ڽڬڠݢۏڔڎڃ' for char in src):\n",
    "                prefix = 'terjemah Jawi ke Bahasa Melayu: '\n",
    "            else:\n",
    "                prefix = 'terjemah Inggeris ke Bahasa Melayu: '\n",
    "            \n",
    "            example['src'] = prefix + src\n",
    "            return example\n",
    "        \n",
    "        # Apply prefixes to first 60k samples for faster processing\n",
    "        print(\"🏷️ Adding language prefixes...\")\n",
    "        dataset = dataset.select(range(60000)).map(add_prefix)\n",
    "        \n",
    "        print(f\"✅ Processed dataset: {len(dataset):,} samples\")\n",
    "        return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading from HF: {e}\")\n",
    "        return create_fallback_dataset()\n",
    "\n",
    "def create_fallback_dataset():\n",
    "    \"\"\"Create a fallback dataset if HF loading fails\"\"\"\n",
    "    print(\"🔄 Creating fallback dataset...\")\n",
    "    \n",
    "    # Sample translation pairs for immediate training\n",
    "    test_data = {\n",
    "        \"src\": [\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Hello, how are you?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Good morning everyone\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: What is your name?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Thank you very much\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Where is the restaurant?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: I would like to order food\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: How much does this cost?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Can you help me please?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: What time is it now?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: I need to go to the hospital\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: The weather is very nice today\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: How do I get to the airport?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: What is this for?\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: I don't understand\",\n",
    "            \"terjemah Inggeris ke Bahasa Melayu: Please speak slowly\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"Hello, apa khabar?\",\n",
    "            \"Selamat pagi semua\",\n",
    "            \"Siapa nama awak?\",\n",
    "            \"Terima kasih banyak-banyak\",\n",
    "            \"Mana restoran?\",\n",
    "            \"Saya nak order makanan\",\n",
    "            \"Berapa harga ini?\",\n",
    "            \"Boleh tolong saya?\",\n",
    "            \"Pukul berapa sekarang?\",\n",
    "            \"Saya perlu pergi ke hospital\",\n",
    "            \"Cuaca hari ini sangat cantik\",\n",
    "            \"Macam mana nak pergi ke lapangan terbang?\",\n",
    "            \"Ini untuk apa?\",\n",
    "            \"Saya tidak faham\",\n",
    "            \"Tolong cakap perlahan-lahan\",\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Create larger dataset by repeating\n",
    "    expanded_src = []\n",
    "    expanded_tgt = []\n",
    "    for i in range(100):  # 1500 samples total\n",
    "        expanded_src.extend(test_data[\"src\"])\n",
    "        expanded_tgt.extend(test_data[\"tgt\"])\n",
    "    \n",
    "    dataset = Dataset.from_dict({\n",
    "        \"src\": expanded_src,\n",
    "        \"tgt\": expanded_tgt\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ Fallback dataset: {len(dataset):,} samples\")\n",
    "    return dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_fresh_dataset()\n",
    "\n",
    "# Split into train/test\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Use appropriate sizes\n",
    "train_size = min(50000, len(split_dataset[\"train\"]))\n",
    "test_size = min(5000, len(split_dataset[\"test\"]))\n",
    "\n",
    "train_subset = split_dataset[\"train\"].select(range(train_size))\n",
    "test_subset = split_dataset[\"test\"].select(range(test_size))\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"test\": test_subset\n",
    "})\n",
    "\n",
    "print(f\"🎯 Final dataset: {len(train_subset):,} train, {len(test_subset):,} test\")\n",
    "\n",
    "print(\"🔧 Loading model and tokenizer...\")\n",
    "\n",
    "# Clear GPU memory first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load model with memory optimizations\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float16,  # Load in half precision\n",
    ")\n",
    "\n",
    "print(\"✅ Model and tokenizer loaded successfully!\")\n",
    "\n",
    "def check_training_data():\n",
    "    \"\"\"Check if training data looks correct\"\"\"\n",
    "    print(\"🔍 Checking training data samples...\")\n",
    "    \n",
    "    sample = final_dataset[\"train\"][0]\n",
    "    input_text = sample[\"src\"]\n",
    "    target_text = sample[\"tgt\"]\n",
    "    \n",
    "    print(f\"📝 Input: {input_text[:100]}...\")\n",
    "    print(f\"🎯 Target: {target_text[:100]}...\")\n",
    "    print(f\"📊 Input length: {len(input_text)}, Target length: {len(target_text)}\")\n",
    "\n",
    "check_training_data()\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize the examples\"\"\"\n",
    "    inputs = [str(x) for x in examples[\"src\"]]\n",
    "    targets = [str(x) for x in examples[\"tgt\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"🔹 Tokenizing dataset...\")\n",
    "tokenized_dataset = final_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=final_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"✅ Tokenization completed!\")\n",
    "\n",
    "# === DATA COLLATOR ===\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute BLEU score for evaluation - WITH OVERFLOW PROTECTION\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    try:\n",
    "        # **FIX: Clip predictions to valid token ID range**\n",
    "        predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
    "        \n",
    "        # Replace -100 with pad token id in labels\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        # Decode predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Filter and clean\n",
    "        filtered_preds = []\n",
    "        filtered_refs = []\n",
    "        \n",
    "        for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "            pred_clean = pred.strip()\n",
    "            ref_clean = ref.strip()\n",
    "            \n",
    "            if pred_clean and ref_clean:\n",
    "                filtered_preds.append(pred_clean)\n",
    "                filtered_refs.append(ref_clean)\n",
    "        \n",
    "        if not filtered_preds:\n",
    "            return {\"bleu\": 0.0, \"gen_len\": 0.0}\n",
    "        \n",
    "        # Compute BLEU\n",
    "        bleu_scorer = BLEU(tokenize='13a')\n",
    "        bleu_result = bleu_scorer.corpus_score(filtered_preds, [filtered_refs])\n",
    "        \n",
    "        # Compute average generation length\n",
    "        prediction_lens = [len(pred.split()) for pred in filtered_preds]\n",
    "        avg_gen_len = np.mean(prediction_lens) if prediction_lens else 0.0\n",
    "        \n",
    "        print(f\"✅ BLEU: {bleu_result.score:.4f}, Samples: {len(filtered_preds)}, Avg Len: {avg_gen_len:.1f}\")\n",
    "        \n",
    "        return {\"bleu\": bleu_result.score, \"gen_len\": avg_gen_len}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ BLEU calculation failed: {e}\")\n",
    "        return {\"bleu\": 0.0, \"gen_len\": 0.0}\n",
    "\n",
    "def check_model_outputs():\n",
    "    \"\"\"Check if model is producing reasonable outputs\"\"\"\n",
    "    print(\"🔍 Checking model outputs...\")\n",
    "    \n",
    "    # Move model to GPU first\n",
    "    model.to(\"cuda\")\n",
    "    print(\"✅ Model moved to GPU\")\n",
    "    \n",
    "    # Test with a simple example\n",
    "    test_input = \"terjemah Inggeris ke Bahasa Melayu: Hello world\"\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\", max_length=128, truncation=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=50,\n",
    "            num_beams=1,  # Use greedy for quick test\n",
    "            # Remove early_stopping for greedy search\n",
    "        )\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"📝 Test input: {test_input}\")\n",
    "    print(f\"🎯 Model output: {prediction}\")\n",
    "    \n",
    "    # Check if output is reasonable\n",
    "    if len(prediction.strip()) > 0 and prediction != test_input:\n",
    "        print(\"✅ Model is producing different outputs\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ Model output issue detected\")\n",
    "        return False\n",
    "\n",
    "# Run the check\n",
    "check_model_outputs()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # TRAINING - ULTRA STABLE\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,           # Mild regularization\n",
    "    warmup_steps=1000,           # More warmup\n",
    "    max_grad_norm=0.5,           # Conservative gradient clipping\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",  \n",
    "    eval_steps=EVAL_STEPS,       # 200\n",
    "    predict_with_generate=True,\n",
    "    \n",
    "    # Saving - FIXED: Make save_steps multiple of eval_steps\n",
    "    save_strategy=\"steps\",  \n",
    "    save_steps=400,              # ← CHANGED from 500 to 400 (multiple of 200)\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_strategy=\"steps\",  \n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"nanot5-small-stable-run\",\n",
    "    \n",
    "    # Optimization - STABILITY FIRST\n",
    "    fp16=False,                 # NO mixed precision\n",
    "    bf16=False,                 # NO mixed precision  \n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[DebugCallback()]  # Add debug callback\n",
    ")\n",
    "\n",
    "train_samples = len(tokenized_dataset[\"train\"])\n",
    "effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "steps_per_epoch = train_samples // effective_batch_size\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "\n",
    "print(f\"🎯 FINAL TRAINING CONFIGURATION:\")\n",
    "print(f\"   • Model: {MODEL_NAME}\")\n",
    "print(f\"   • Dataset: {train_samples:,} train, {len(tokenized_dataset['test']):,} test\")\n",
    "print(f\"   • Batch Size: {BATCH_SIZE} (effective: {effective_batch_size})\")\n",
    "print(f\"   • Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   • Epochs: {EPOCHS}\")\n",
    "print(f\"   • Total Steps: ~{total_steps:,}\")\n",
    "print(f\"   • Evaluation: Every {EVAL_STEPS} steps\")\n",
    "print(f\"   • Target BLEU: >10.0\")\n",
    "\n",
    "# Clear everything first\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"🔄 Loading nanot5-small with stability checks...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load in FP32 for maximum stability\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float32,  # FP32 ONLY for stability\n",
    ")\n",
    "\n",
    "print(\"✅ nanot5-small loaded successfully!\")\n",
    "print(f\"📊 Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "def quick_stability_check():\n",
    "    \"\"\"Quick stability test for T5 models\"\"\"\n",
    "    print(\"🔍 Quick stability check for T5...\")\n",
    "    \n",
    "    try:\n",
    "        # Move model to GPU\n",
    "        model.to(\"cuda\")\n",
    "        \n",
    "        # Test input and target (T5 needs both for forward pass)\n",
    "        test_input = \"terjemah Inggeris ke Bahasa Melayu: Hello world\"\n",
    "        test_target = \"Halo dunia\"\n",
    "        \n",
    "        # Tokenize both input and target\n",
    "        inputs = tokenizer(test_input, return_tensors=\"pt\", max_length=64, truncation=True, padding=\"max_length\")\n",
    "        labels = tokenizer(test_target, return_tensors=\"pt\", max_length=64, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        # Remove token_type_ids for T5\n",
    "        for d in [inputs, labels]:\n",
    "            if 'token_type_ids' in d:\n",
    "                del d['token_type_ids']\n",
    "        \n",
    "        # Move to GPU\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        labels = labels['input_ids'].to(\"cuda\")\n",
    "        \n",
    "        # For T5 forward pass with loss, we need to provide labels\n",
    "        inputs['labels'] = labels\n",
    "        \n",
    "        # Test forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss.item() if outputs.loss is not None else \"N/A\"\n",
    "        \n",
    "        print(f\"📊 Forward loss: {loss}\")\n",
    "        \n",
    "        # Test generation separately (this doesn't need labels)\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                max_length=32,\n",
    "                num_beams=1\n",
    "            )\n",
    "            prediction = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"🎯 Test generation: {prediction}\")\n",
    "        \n",
    "        # Check if reasonable\n",
    "        if isinstance(loss, float) and loss < 1000 and loss > 0:\n",
    "            print(\"✅ Stability check passed!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Loss issue detected\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during stability check: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the check\n",
    "if quick_stability_check():\n",
    "    print(\"🎉 Ready for stable training!\")\n",
    "else:\n",
    "    print(\"🚨 Stability issues detected!\")\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset_path = \"/workspace/malaysian_english_stage2_part1\"\n",
    "\n",
    "print(\"🔄 Attempting to recreate from parquet file...\")\n",
    "try:\n",
    "    # Load the parquet file that's in your directory\n",
    "    parquet_path = os.path.join(dataset_path, \"malaysian_translation_stage2_part1.parquet\")\n",
    "    \n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"📖 Loading parquet file: {parquet_path}\")\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        print(f\"✅ Loaded parquet: {len(df):,} rows, columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Convert to Hugging Face dataset\n",
    "        full_dataset = Dataset.from_pandas(df)\n",
    "        print(f\"✅ Created dataset: {len(full_dataset):,} samples\")\n",
    "        \n",
    "        # Save it properly\n",
    "        print(\"💾 Saving as proper dataset...\")\n",
    "        full_dataset.save_to_disk(dataset_path + \"_fixed\")\n",
    "        print(\"✅ Dataset saved successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Parquet file not found at: {parquet_path}\")\n",
    "        print(\"📁 Available files:\")\n",
    "        for file in os.listdir(dataset_path):\n",
    "            print(f\"   - {file}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error recreating from parquet: {e}\")\n",
    "\n",
    "print(\"🚀 STARTING TRAINING...\")\n",
    "torch.cuda.empty_cache()\n",
    "training_result = trainer.train()\n",
    "\n",
    "# Start training (this will run for several hours)\n",
    "training_result = trainer.train()\n",
    "\n",
    "# === SAVE MODEL ===\n",
    "print(\"💾 Saving model...\")\n",
    "trainer.save_model()\n",
    "\n",
    "print(f\"📁 Model saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# === FINAL EVALUATION ===\n",
    "print(\"🔍 Running final evaluation...\")\n",
    "final_metrics = trainer.evaluate()\n",
    "print(f\"📊 Final BLEU score: {final_metrics.get('eval_bleu', 0):.2f}\")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"✅ TRAINING COMPLETED!\")\n",
    "\n",
    "okie now is to fix the full dataset and remove those debug code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
